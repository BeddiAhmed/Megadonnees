{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ECN 6360 - Mégadonnées: methodes et applications\n",
        "\n",
        "## Question 1: K plus proches voisins\n",
        "\n",
        "Le tableau ci-dessous contient 6 observations d'entrainement pour 3 prédicteurs et une réponse qualitative.\n",
        "\n",
        "\n",
        "\n",
        "\\begin{array}{c} \\hline\n",
        "Observation & X_1 & X_2 & X_3 & Y\\\\ \\hline\n",
        "1 & 0 & 3 & 0 & rouge \\\\\n",
        "2 & 2 & 0 & 0 & rouge\\\\\n",
        "3 & 0 & 1 & 3 & rouge\\\\\n",
        "4 & 0 & 1 & 2 & vert \\\\\n",
        "5 & -1 & 0 & 1 & vert\\\\\n",
        "6 & 1 & 1 & 1 & rouge \\\\ \\hline\n",
        "\\end{array}\n",
        "<br>\n",
        "On veut utiliser cet échantillon pour prédire la valeur de Y quand $X_1 = X_2 = X_3 = 0$ à l'aide de la méthode des K plus proches voisins.\n",
        "<ol type=\"a\">\n",
        "  <li>Calculez la distance euclidienne entre chaque observation et l'observation test $X_1 = X_2 = X_3 = 0$</li>\n",
        "  <li>Quelle est votre prédiction si $K = 1$? Justifiez votre réponse.</li>\n",
        "  <li>Quelle est votre prédiction si $K = 3$? Justifiez votre réponse.</li>\n",
        "  <li>Si la frontière de décision de Bayes est très nonlinéaire, la meilleure valeur pour $K$ devrait-elle être grande ou petite? Pourquoi?</li>\n",
        "</ol>\n",
        "\n",
        "### Solution\n",
        "\n",
        "<ol type=\"a\">\n",
        "  <li>Soit $X_0 = (X_{1}, X_{2}, X_{3}) = (0,0,0)$ l'observation test et soit $X_j = (X_{j1}, X_{j2}, X_{j3})$ l'observation d'entrainement $j$ où $X_{ji}$ est la valeur de $X_{i}$ dans l'observation d'entrainement $j$. $i = 1, 2, 3$ et $j = 1,...,6$.<br>\n",
        "   La distance euclidienne entre chaque observation j et l'observation test est donnée par:\n",
        "\n",
        "  $||X_{j} - X_{0}|| = \\sqrt{(X_{j1} - 0)^2 + (X_{j2} - 0)^2 + (X_{j3} - 0)^2}$<br><br>\n",
        "   On utilise cette formule pour chaque observation: <br><br>\n",
        "   Obs. 1: $\\sqrt{(0 - 0)^2 + (3 - 0)^2 + (0 - 0)^2} = 3$<br><br>\n",
        "   Obs. 2: $\\sqrt{(2 - 0)^2 + (0 - 0)^2 + (0 - 0)^2} = 2$<br><br>\n",
        "   Obs. 3: $\\sqrt{(0 - 0)^2 + (1 - 0)^2 + (3 - 0)^2} = \\sqrt{10}$<br><br>\n",
        "   Obs. 4: $\\sqrt{(0 - 0)^2 + (1 - 0)^2 + (2 - 0)^2} = \\sqrt{5}$<br><br>\n",
        "   Obs. 5: $\\sqrt{(-1 - 0)^2 + (0 - 0)^2 + (1 - 0)^2} = \\sqrt{2}$<br><br>\n",
        "   Obs. 6: $\\sqrt{(1 - 0)^2 + (1 - 0)^2 + (1 - 0)^2} = \\sqrt{3}$<br><br>\n",
        "\n",
        "   \n",
        "\n",
        "   \\begin{array}{c} \\hline\n",
        "Observation & X_1 & X_2 & X_3 & Y & ||X_{j} - X_{0}||\\\\ \\hline\n",
        "1 & 0 & 3 & 0 & rouge &  3\\\\\n",
        "2 & 2 & 0 & 0 & rouge & 2\\\\\n",
        "3 & 0 & 1 & 3 & rouge & \\sqrt{10}\\\\\n",
        "4 & 0 & 1 & 2 & vert & \\sqrt{5}\\\\\n",
        "5 & -1 & 0 & 1 & vert & \\sqrt{2}\\\\\n",
        "6 & 1 & 1 & 1 & rouge & \\sqrt{3}\\\\ \\hline\n",
        "\\end{array}\n",
        "<br></li>\n",
        "  <li>$K = 1$, on identifie l'observation la plus proche de l'observation test, c'est-à-dire l'observation $j$ avec la distance $||X_{j} - X_{0}||$ la plus petite.<br>\n",
        "  Ici, observation $5$ est la plus proche de $X_0 = (X_{1}, X_{2}, X_{3}) = (0,0,0)$ et on observe $Y = vert$. Alors, <br>\n",
        "  $P(Y = vert | X = X_{0}) = 1$<br><br>\n",
        "  Donc, notre prediction est que $Y$ est $vert$ quand $X_{1} = X_{2} = X_{3} = 0$.  </li><br>\n",
        "  <li>$K = 3$, on identifie les trois observations les plus proches de l'observation test.<br>\n",
        "  Ici, les observations $2, 5,$ et $6$ sont les plus proches de $X_0 = (X_{1}, X_{2}, X_{3}) = (0,0,0)$ et on observe $(Y_{2} = rouge, Y_{5} = vert, Y_{6} = rouge)$. Alors, <br>\n",
        "    \\begin{align}\n",
        "        P(Y = vert \\mid X = X_{0}) = \\frac{1}{K} \\sum_{j=2,5,6} I(Y_j = vert).\n",
        "    \\end{align}\n",
        "    \\begin{align}\n",
        "         = \\frac{1}{3} (1) = \\frac{1}{3}\n",
        "    \\end{align}\n",
        "  <br><br>\n",
        "  \\begin{align}\n",
        "        P(Y = rouge \\mid X = X_{0}) = \\frac{1}{K} \\sum_{j=2,5,6} I(Y_j = rouge).\n",
        "    \\end{align}\n",
        "    \\begin{align}\n",
        "         = \\frac{1}{3} (2) = \\frac{2}{3}\n",
        "    \\end{align}\n",
        "  <br><br>\n",
        "  Donc, notre prediction est que $Y$ est $rouge$ quand $X_{1} = X_{2} = X_{3} = 0$. </li><br>\n",
        "  <li>Si la frontière de décision de Bayes est très nonlinéaire, la meilleure valeur pour $K$ devrait être petite. La raison pour ceci est que KNN est plus flexible avec un $K$ très petit qu'avec un K très grand et donc va estimer une frontière de décision de Bayes très nonlinéaire. </li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "lFs7LJDeEK-C"
      }
    }
  ]
}